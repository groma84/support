<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Deep Learning / List.community</title><script type="text/javascript">!function(i){if(i.search){var a={};i.search.slice(1).split("&").forEach(function(i){var l=i.split("=");a[l[0]]=l.slice(1).join("=").replace(/~and~/g,"&")}),void 0!==a.p&&window.history.replaceState(null,null,i.pathname.slice(0,-1)+(a.p||"")+(a.q?"?"+a.q:"")+i.hash)}}(window.location)</script><script src="https://cdn.ravenjs.com/3.21.0/raven.min.js" crossorigin="anonymous"></script><link href="https://fonts.googleapis.com/css?family=Karla:400,700,700i" rel="stylesheet"><link href="/static/css/main.b8c1ca79.css" rel="stylesheet"><link rel="icon" type="image/png" href="/favicon.png?size=32" data-react-helmet="true"><meta name="description" content="List.community is an easy way to browse curated lists on GitHub." data-react-helmet="true"><meta property="og:url" content="https://list.community" data-react-helmet="true"><meta property="og:site_name" content="List.community" data-react-helmet="true"><meta property="og:title" content="Deep Learning / List.community" data-react-helmet="true"><meta property="og:description" content="List.community is an easy way to browse curated lists on GitHub." data-react-helmet="true"><meta property="og:image" content="https://list.community/avatar.png" data-react-helmet="true"><meta name="twitter:card" content="summary" data-react-helmet="true"><meta name="twitter:title" content="Deep Learning / List.community" data-react-helmet="true"><meta name="twitter:description" content="List.community is an easy way to browse curated lists on GitHub." data-react-helmet="true"><meta name="twitter:image" content="https://list.community/avatar.png" data-react-helmet="true"><link rel="preload" as="script" href="/static/js/main.ce8adbb6.js"></head><body><div id="root"><div class="overflow-x-hidden bg-white min-h-screen pt-15 lg:pt-0"><div class="lg:mr-80"><div class="fixed pin-x pin-t z-20 lg:mr-80"><header class="select-none bg-black shadow-md text-black" style="background:#fff"><div class="max-w-xl mx-auto flex items-center h-15 pb-1 px-1 xl:px-5"><a class="link-reset p-3 mr-2" href="/"><h1 class="text-xl font-semibold font-sans tracking-wide">List.community</h1></a><div class="text-xl hidden xl:block truncate">ChristosChristofidis/awesome-deep-learning</div><div class="flex-1"></div><a href="https://github.com/ChristosChristofidis/awesome-deep-learning" class="link-reset p-3 pt-4 hidden sm:block flex-none">Submit a resource</a><a href="https://github.com/ChristosChristofidis/awesome-deep-learning/graphs/contributors" class="link-reset p-3 pt-4 hidden sm:block flex-none">Curators</a><a href="https://github.com/ChristosChristofidis/awesome-deep-learning" class="link-reset p-3 pt-4 leading-none flex-none"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path d="m8 0c-4.42 0-8 3.582-8 8 0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.225.483-2.695-1.073-2.695-1.073-.364-.923-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.713 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117 0 0 .67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.273.82 2.147 0 3.073-1.87 3.75-3.65 3.947.28.24.54.731.54 1.48 0 1.071-.01 1.931-.01 2.191 0 .21.14.46.55.38 3.201-1.049 5.491-4.049 5.491-7.579 0-4.418-3.582-8-8-8"></path></svg></a><div class="cursor-pointer lg:hidden p-3 pt-4 leading-none flex-none"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="16" height="16"><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z"></path></svg></div></div></header></div><div class="w-full lg:pt-15"><div id="table-of-contents"></div><div id="start-of-content"></div><div id="contents"></div><div id="readme" class="markdown-body p-4 xl:p-8 max-w-xl mx-auto"><h1><a href="#awesome-deep-learning-" aria-hidden="true" class="anchor" id="user-content-awesome-deep-learning-"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Awesome Deep Learning <a target="_blank" rel="noopener noreferrer" href="https://github.com/sindresorhus/awesome"><img alt="Awesome" target="_blank" rel="noopener noreferrer" src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%"></a></h1>
<h2 style="display:none"><a href="#table-of-contents" aria-hidden="true" class="anchor" id="user-content-table-of-contents"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Table of Contents</h2>
<ul style="display:none">
<li>
<p><strong><a href="#free-online-books">Free Online Books</a></strong></p>
</li>
<li>
<p><strong><a href="#courses">Courses</a></strong></p>
</li>
<li>
<p><strong><a href="#videos-and-lectures">Videos and Lectures</a></strong></p>
</li>
<li>
<p><strong><a href="#papers">Papers</a></strong></p>
</li>
<li>
<p><strong><a href="#tutorials">Tutorials</a></strong></p>
</li>
<li>
<p><strong><a href="#researchers">Researchers</a></strong></p>
</li>
<li>
<p><strong><a href="#websites">WebSites</a></strong></p>
</li>
<li>
<p><strong><a href="#datasets">Datasets</a></strong></p>
</li>
<li>
<p><strong><a href="#Conferences">Conferences</a></strong></p>
</li>
<li>
<p><strong><a href="#frameworks">Frameworks</a></strong></p>
</li>
<li>
<p><strong><a href="#miscellaneous">Miscellaneous</a></strong></p>
</li>
<li>
<p><strong><a href="#contributing">Contributing</a></strong></p>
</li>
</ul>
<h3><a href="#free-online-books" aria-hidden="true" class="anchor" id="user-content-free-online-books"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Free Online Books</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="http://www.deeplearningbook.org/">Deep Learning</a> by Yoshua Bengio, Ian Goodfellow and Aaron Courville  (05/07/2015)</li>
<li><a target="_blank" rel="nofollow" href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> by  Michael Nielsen (Dec 2014)</li>
<li><a target="_blank" rel="nofollow" href="http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf">Deep Learning</a> by Microsoft Research (2013)</li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.net/tutorial/deeplearning.pdf">Deep Learning Tutorial</a> by LISA lab, University of Montreal (Jan 6 2015)</li>
<li><a id="user-content-karpathy/neuraltalk" target="_blank" rel="noopener noreferrer" href="https://github.com/karpathy/neuraltalk">neuraltalk</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">270 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span> by Andrej Karpathy : numpy-based RNN/LSTM implementation</li>
<li><a target="_blank" rel="nofollow" href="https://svn-d1.mpi-inf.mpg.de/AG1/MultiCoreLab/papers/ebook-fuzzy-mitchell-99.pdf">An introduction to genetic algorithms</a></li>
<li><a target="_blank" rel="nofollow" href="http://aima.cs.berkeley.edu/">Artificial Intelligence: A Modern Approach</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1404.7828v4.pdf">Deep Learning in Neural Networks: An Overview</a></li>
</ol>
<h3><a href="#courses" aria-hidden="true" class="anchor" id="user-content-courses"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Courses</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="https://class.coursera.org/ml-005">Machine Learning - Stanford</a> by Andrew Ng in Coursera (2010-2014)</li>
<li><a target="_blank" rel="nofollow" href="http://work.caltech.edu/lectures.html">Machine Learning - Caltech</a> by Yaser Abu-Mostafa (2012-2014)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.cmu.edu/%7Etom/10701_sp11/lectures.shtml">Machine Learning - Carnegie Mellon</a> by Tom Mitchell (Spring 2011)</li>
<li><a target="_blank" rel="nofollow" href="https://class.coursera.org/neuralnets-2012-001">Neural Networks for Machine Learning</a> by Geoffrey Hinton in Coursera (2012)</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Neural networks class</a> by Hugo Larochelle from Université de Sherbrooke (2013)</li>
<li><a target="_blank" rel="nofollow" href="http://cilvr.cs.nyu.edu/doku.php?id=deeplearning:slides:start">Deep Learning Course</a> by CILVR lab @ NYU (2014)</li>
<li><a target="_blank" rel="nofollow" href="https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/courseware/">A.I - Berkeley</a> by Dan Klein and Pieter Abbeel (2013)</li>
<li><a target="_blank" rel="nofollow" href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/">A.I - MIT</a> by Patrick Henry Winston (2010)</li>
<li><a target="_blank" rel="nofollow" href="http://web.mit.edu/course/other/i2course/www/vision_and_learning_fall_2013.html">Vision and learning - computers and brains</a> by Shimon Ullman, Tomaso Poggio, Ethan Meyers @ MIT (2013)</li>
<li><a target="_blank" rel="nofollow" href="http://vision.stanford.edu/teaching/cs231n/syllabus.html">Convolutional Neural Networks for Visual Recognition - Stanford</a> by Fei-Fei Li, Andrej Karpathy (2017)</li>
<li><a target="_blank" rel="nofollow" href="http://cs224d.stanford.edu/">Deep Learning for Natural Language Processing - Stanford</a></li>
<li><a target="_blank" rel="nofollow" href="http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html">Neural Networks - usherbrooke</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/">Machine Learning - Oxford</a> (2014-2015)</li>
<li><a target="_blank" rel="nofollow" href="https://developer.nvidia.com/deep-learning-courses">Deep Learning - Nvidia</a> (2015)</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/playlist?list=PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA">Graduate Summer School: Deep Learning, Feature Learning</a> by Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrew Ng, Nando de Freitas and several others @ IPAM, UCLA (2012)</li>
<li><a target="_blank" rel="nofollow" href="https://www.udacity.com/course/deep-learning--ud730">Deep Learning - Udacity/Google</a> by Vincent Vanhoucke and Arpan Chakraborty (2016)</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE">Deep Learning - UWaterloo</a> by Prof. Ali Ghodsi at University of Waterloo (2015)</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=azaLcvuql_g&amp;list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r">Statistical Machine Learning - CMU</a> by Prof. Larry Wasserman</li>
<li><a target="_blank" rel="nofollow" href="https://www.college-de-france.fr/site/en-yann-lecun/course-2015-2016.htm">Deep Learning Course</a> by Yann LeCun (2016)</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm">Designing, Visualizing and Understanding Deep Neural Networks-UC Berkeley</a></li>
<li><a target="_blank" rel="nofollow" href="http://uvadlc.github.io">UVA Deep Learning Course</a> MSc in Artificial Intelligence for the University of Amsterdam.</li>
<li><a target="_blank" rel="nofollow" href="http://selfdrivingcars.mit.edu/">MIT 6.S094: Deep Learning for Self-Driving Cars</a></li>
<li><a target="_blank" rel="nofollow" href="http://introtodeeplearning.com/">MIT 6.S191: Introduction to Deep Learning</a></li>
<li><a target="_blank" rel="nofollow" href="http://rll.berkeley.edu/deeprlcourse/">Berkeley CS 294: Deep Reinforcement Learning</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.manning.com/livevideo/keras-in-motion">Keras in Motion video course</a></li>
<li><a target="_blank" rel="nofollow" href="http://course.fast.ai/">Practical Deep Learning For Coders</a> by Jeremy Howard - Fast.ai</li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.cs.cmu.edu/">Introduction to Deep Learning</a> by Prof. Bhiksha Raj (2017)</li>
</ol>
<h3><a href="#videos-and-lectures" aria-hidden="true" class="anchor" id="user-content-videos-and-lectures"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Videos and Lectures</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=RIkxVci-R4k">How To Create A Mind</a> By Ray Kurzweil</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=n1ViNeWhC24">Deep Learning, Self-Taught Learning and Unsupervised Feature Learning</a> By Andrew Ng</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=vShMxxqtDDs&amp;index=3&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT">Recent Developments in Deep Learning</a> By Geoff Hinton</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=sc-KbuZqGkI">The Unreasonable Effectiveness of Deep Learning</a> by Yann LeCun</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=4xsVFLnHC_0">Deep Learning of Representations</a> by Yoshua bengio</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=6ufPpZDmPKA">Principles of Hierarchical Temporal Memory</a> by Jeff Hawkins</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=2QJi0ArLq7s&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT">Machine Learning Discussion Group - Deep Learning w/ Stanford AI Lab</a> by Adam Coates</li>
<li><a target="_blank" rel="nofollow" href="http://vimeo.com/80821560">Making Sense of the World with Deep Learning</a> By Adam Coates</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=wZfVBwOO0-k">Demystifying Unsupervised Feature Learning </a> By Adam Coates</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=3boKlkPBckA">Visual Perception with Deep Learning</a> By Yann LeCun</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=AyzOUbkUf3M">The Next Generation of Neural Networks</a> By Geoffrey Hinton at GoogleTechTalks</li>
<li><a target="_blank" rel="nofollow" href="http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn">The wonderful and terrifying implications of computers that can learn</a> By Jeremy Howard at TEDxBrussels</li>
<li><a target="_blank" rel="nofollow" href="http://web.stanford.edu/class/cs294a/handouts.html">Unsupervised Deep Learning - Stanford</a> by Andrew Ng in Stanford (2011)</li>
<li><a target="_blank" rel="nofollow" href="http://web.stanford.edu/class/cs224n/handouts/">Natural Language Processing</a> By Chris Manning in Stanford</li>
<li><a target="_blank" rel="nofollow" href="http://googleresearch.blogspot.com/2015/09/a-beginners-guide-to-deep-neural.html">A beginners Guide to Deep Neural Networks</a> By Natalie Hammel and Lorraine Yurshansky</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=czLI3oLDe8M">Deep Learning: Intelligence from Big Data</a> by Steve Jurvetson (and panel) at VLAB in Stanford.</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=FoO8qDB8gUU">Introduction to Artificial Neural Networks and Deep Learning</a> by Leo Isikdogan at Motorola Mobility HQ</li>
<li><a target="_blank" rel="nofollow" href="https://nips.cc/Conferences/2016/Schedule">NIPS 2016 lecture and workshop videos</a> - NIPS 2016</li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=oS5fz_mHVz0&amp;list=PLWKotBjTDoLj3rXBL-nEIPRN9V3a9Cx07">Deep Learning Crash Course</a>: a series of mini-lectures by Leo Isikdogan on YouTube (2018)</li>
</ol>
<h3><a href="#papers" aria-hidden="true" class="anchor" id="user-content-papers"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Papers</h3>
<p><em>You can also find the most cited deep learning papers from <a id="user-content-terryum/awesome-deep-learning-papers" target="_blank" rel="noopener noreferrer" href="https://github.com/terryum/awesome-deep-learning-papers">here</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">10k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></em></p>
<ol>
<li><a target="_blank" rel="nofollow" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Ehinton/absps/esann-deep-final.pdf">Using Very Deep Autoencoders for Content Based Image Retrieval</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.iro.umontreal.ca/%7Elisa/pointeurs/TR1312.pdf">Learning Deep Architectures for AI</a></li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.cs.cmu.edu/">CMU’s list of papers</a></li>
<li><a target="_blank" rel="nofollow" href="http://nlp.stanford.edu/%7Esocherr/pa4_ner.pdf">Neural Networks for Named Entity Recognition</a> <a target="_blank" rel="nofollow" href="http://nlp.stanford.edu/%7Esocherr/pa4-ner.zip">zip</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.iro.umontreal.ca/%7Ebengioy/papers/YB-tricks.pdf">Training tricks by YB</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Ehinton/deeprefs.html">Geoff Hinton's reading list (all papers)</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Egraves/preprint.pdf">Supervised Sequence Labelling with Recurrent Neural Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.fit.vutbr.cz/%7Eimikolov/rnnlm/thesis.pdf">Statistical Language Models based on Neural Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.utoronto.ca/%7Eilya/pubs/ilya_sutskever_phd_thesis.pdf">Training Recurrent Neural Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://nlp.stanford.edu/%7Esocherr/thesis.pdf">Recursive Deep Learning for Natural Language Processing and Computer Vision</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.di.ufpe.br/%7Efnj/RNA/bibliografia/BRNN.pdf">Bi-directional RNN</a></li>
<li><a target="_blank" rel="nofollow" href="http://web.eecs.utk.edu/%7Eitamar/courses/ECE-692/Bobby_paper1.pdf">LSTM</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1406.1078v3.pdf">GRU - Gated Recurrent Unit</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1502.02367v3.pdf">GFRNN</a> <a target="_blank" rel="nofollow" href="http://jmlr.org/proceedings/papers/v37/chung15.pdf">.</a> <a target="_blank" rel="nofollow" href="http://jmlr.org/proceedings/papers/v37/chung15-supp.pdf">.</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1503.04069v1.pdf">LSTM: A Search Space Odyssey</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1506.00019v1.pdf">A Critical Review of Recurrent Neural Networks for Sequence Learning</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1506.02078v1.pdf">Visualizing and Understanding Recurrent Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Wojciech Zaremba, Ilya Sutskever, An Empirical Exploration of Recurrent Network Architectures</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Recurrent Neural Network based Language Model</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf">Extensions of Recurrent Neural Network Language Model</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.fit.vutbr.cz/%7Eimikolov/rnnlm/ApplicationOfRNNinMeetingRecognition_IS2011.pdf">Recurrent Neural Network based Language Modeling in Meeting Recognition</a></li>
<li><a target="_blank" rel="nofollow" href="http://cs224d.stanford.edu/papers/maas_paper.pdf">Deep Neural Networks for Acoustic Modeling in Speech Recognition</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Efritz/absps/RNN13.pdf">Speech Recognition with Deep Recurrent Neural Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1505.00521v1">Reinforcement Learning Neural Turing Machines</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1406.1078v3.pdf">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a target="_blank" rel="nofollow" href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Google - Sequence to Sequence  Learning with Neural Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1410.3916v10">Memory Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1507.01273v1">Policy Learning with Continuous Memory States for Partially Observed Robotic Control</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1505.01861v1.pdf">Microsoft - Jointly Modeling Embedding and Translation to Bridge Video and Language</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1410.5401v2.pdf">Neural Turing Machines</a></li>
<li><a target="_blank" rel="nofollow" href="http://arxiv.org/pdf/1506.07285v1.pdf">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf">Mastering the Game of Go with Deep Neural Networks and Tree Search</a></li>
<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1502.03167">Batch Normalization</a></li>
<li><a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/1512.03385v1.pdf">Residual Learning</a></li>
<li><a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/1611.07004v1.pdf">Image-to-Image Translation with Conditional Adversarial Networks</a></li>
<li><a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/1611.07004v1.pdf">Berkeley AI Research (BAIR) Laboratory</a></li>
<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1704.04861">MobileNets by Google</a></li>
<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1706.05739">Cross Audio-Visual Recognition in the Wild Using Deep Learning</a></li>
<li><a target="_blank" rel="nofollow" href="https://arxiv.org/abs/1710.09829">Dynamic Routing Between Capsules</a></li>
<li><a target="_blank" rel="nofollow" href="https://openreview.net/pdf?id=HJWLfGWRb">Matrix Capsules With Em Routing</a></li>
</ol>
<h3><a href="#tutorials" aria-hidden="true" class="anchor" id="user-content-tutorials"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tutorials</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial">UFLDL Tutorial 1</a></li>
<li><a target="_blank" rel="nofollow" href="http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/">UFLDL Tutorial 2</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial">Deep Learning for NLP (without Magic)</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks">A Deep Learning Tutorial: From Perceptrons to Deep Networks</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.metacademy.org/roadmaps/rgrosse/deep_learning">Deep Learning from the Bottom up</a></li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.net/tutorial/deeplearning.pdf">Theano Tutorial</a></li>
<li><a target="_blank" rel="nofollow" href="http://uk.mathworks.com/help/pdf_doc/nnet/nnet_ug.pdf">Neural Networks for Matlab</a></li>
<li><a target="_blank" rel="nofollow" href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/">Using convolutional neural nets to detect facial keypoints tutorial</a></li>
<li><a id="user-content-clementfarabet/ipam-tutorials" target="_blank" rel="noopener noreferrer" href="https://github.com/clementfarabet/ipam-tutorials/tree/master/th_tutorials">Torch7 Tutorials</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">42 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-grey-dark">8 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-josephmisiti/machine-learning-module" target="_blank" rel="noopener noreferrer" href="https://github.com/josephmisiti/machine-learning-module">The Best Machine Learning Tutorials On The Web</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">180 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">84 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="http://www.robots.ox.ac.uk/%7Evgg/practicals/cnn/index.html">VGG Convolutional Neural Networks Practical</a></li>
<li><a id="user-content-nlintz/TensorFlow-Tutorials" target="_blank" rel="noopener noreferrer" href="https://github.com/nlintz/TensorFlow-Tutorials">TensorFlow tutorials</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">4k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-pkmital/tensorflow_tutorials" target="_blank" rel="noopener noreferrer" href="https://github.com/pkmital/tensorflow_tutorials">More TensorFlow tutorials</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">4k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">707 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener noreferrer" href="https://github.com/aymericdamien/TensorFlow-Examples">TensorFlow Python Notebooks</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">13k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">4k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-Vict0rSch/deep_learning" target="_blank" rel="noopener noreferrer" href="https://github.com/Vict0rSch/deep_learning">Keras and Lasagne Deep Learning Tutorials</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">281 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">101 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-guillaume-chevalier/LSTM-Human-Activity-Recognition" target="_blank" rel="noopener noreferrer" href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition">Classification on raw time series in TensorFlow with a LSTM RNN</a></li>
<li><a target="_blank" rel="nofollow" href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/">Using convolutional neural nets to detect facial keypoints tutorial</a></li>
<li><a id="user-content-astorfi/TensorFlow-World" target="_blank" rel="noopener noreferrer" href="https://github.com/astorfi/TensorFlow-World">TensorFlow-World</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">163 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.manning.com/books/grokking-deep-learning">Grokking Deep Learning</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.manning.com/books/deep-learning-for-search">Deep Learning for Search</a></li>
<li><a target="_blank" rel="nofollow" href="https://blog.sicara.com/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511">Keras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder</a></li>
<li><a id="user-content-yunjey/pytorch-tutorial" target="_blank" rel="noopener noreferrer" href="https://github.com/yunjey/pytorch-tutorial">Pytorch Tutorial by Yunjey Choi</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">252 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
</ol>
<h2><a href="#researchers" aria-hidden="true" class="anchor" id="user-content-researchers"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Researchers</h2>
<ol>
<li><a target="_blank" rel="nofollow" href="http://aaroncourville.wordpress.com">Aaron Courville</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Easamir/">Abdel-rahman Mohamed</a></li>
<li><a target="_blank" rel="nofollow" href="http://cs.stanford.edu/%7Eacoates/">Adam Coates</a></li>
<li><a target="_blank" rel="nofollow" href="http://research.microsoft.com/en-us/people/alexac/">Alex Acero</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.utoronto.ca/%7Ekriz/index.html"> Alex Krizhevsky </a></li>
<li><a target="_blank" rel="nofollow" href="http://users.ics.aalto.fi/alexilin/"> Alexander Ilin </a></li>
<li><a target="_blank" rel="nofollow" href="http://homepages.inf.ed.ac.uk/amos/"> Amos Storkey </a></li>
<li><a target="_blank" rel="nofollow" href="http://cs.stanford.edu/%7Ekarpathy/"> Andrej Karpathy </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.stanford.edu/%7Easaxe/"> Andrew M. Saxe </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.stanford.edu/people/ang/"> Andrew Ng </a></li>
<li><a target="_blank" rel="nofollow" href="http://research.google.com/pubs/author37792.html"> Andrew W. Senior </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.gatsby.ucl.ac.uk/%7Eamnih/"> Andriy Mnih </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.nyu.edu/%7Enaz/"> Ayse Naz Erkan </a></li>
<li><a target="_blank" rel="nofollow" href="http://reslab.elis.ugent.be/benjamin"> Benjamin Schrauwen </a></li>
<li><a target="_blank" rel="nofollow" href="https://www.cisuc.uc.pt/people/show/2020"> Bernardete Ribeiro </a></li>
<li><a target="_blank" rel="nofollow" href="http://vision.caltech.edu/%7Ebchen3/Site/Bo_David_Chen.html"> Bo David Chen </a></li>
<li><a target="_blank" rel="nofollow" href="http://cs.nyu.edu/%7Eylan/"> Boureau Y-Lan </a></li>
<li><a target="_blank" rel="nofollow" href="http://researcher.watson.ibm.com/researcher/view.php?person=us-bedk"> Brian Kingsbury </a></li>
<li><a target="_blank" rel="nofollow" href="http://nlp.stanford.edu/%7Emanning/"> Christopher Manning </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.clement.farabet.net/"> Clement Farabet </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.idsia.ch/%7Eciresan/"> Dan Claudiu Cireșan </a></li>
<li><a target="_blank" rel="nofollow" href="http://serre-lab.clps.brown.edu/person/david-reichert/"> David Reichert </a></li>
<li><a target="_blank" rel="nofollow" href="http://mil.engr.utk.edu/nmil/member/5.html"> Derek Rose </a></li>
<li><a target="_blank" rel="nofollow" href="http://research.microsoft.com/en-us/people/dongyu/default.aspx"> Dong Yu </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.seas.upenn.edu/%7Ewulsin/"> Drausin Wulsin </a></li>
<li><a target="_blank" rel="nofollow" href="http://music.ece.drexel.edu/people/eschmidt"> Erik M. Schmidt </a></li>
<li><a target="_blank" rel="nofollow" href="https://engineering.purdue.edu/BME/People/viewPersonById?resource_id=71333"> Eugenio Culurciello </a></li>
<li><a target="_blank" rel="nofollow" href="http://research.microsoft.com/en-us/people/fseide/"> Frank Seide </a></li>
<li><a target="_blank" rel="nofollow" href="http://homes.cs.washington.edu/%7Egalen/"> Galen Andrew </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Ehinton/"> Geoffrey Hinton </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Egdahl/"> George Dahl </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.uoguelph.ca/%7Egwtaylor/"> Graham Taylor </a></li>
<li><a target="_blank" rel="nofollow" href="http://gregoire.montavon.name/"> Grégoire Montavon </a></li>
<li><a target="_blank" rel="nofollow" href="http://personal-homepages.mis.mpg.de/montufar/"> Guido Francisco Montúfar </a></li>
<li><a target="_blank" rel="nofollow" href="http://brainlogging.wordpress.com/"> Guillaume Desjardins </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.ais.uni-bonn.de/%7Eschulz/"> Hannes Schulz </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.lri.fr/%7Ehpaugam/"> Hélène Paugam-Moisy </a></li>
<li><a target="_blank" rel="nofollow" href="http://web.eecs.umich.edu/%7Ehonglak/"> Honglak Lee </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.dmi.usherb.ca/%7Elarocheh/index_en.html"> Hugo Larochelle </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Eilya/"> Ilya Sutskever </a></li>
<li><a target="_blank" rel="nofollow" href="http://mil.engr.utk.edu/nmil/member/2.html"> Itamar Arel </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Ejmartens/"> James Martens </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.jasonmorton.com/"> Jason Morton </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.thespermwhale.com/jaseweston/"> Jason Weston </a></li>
<li><a target="_blank" rel="nofollow" href="http://research.google.com/pubs/jeff.html"> Jeff Dean </a></li>
<li><a target="_blank" rel="nofollow" href="http://cs.stanford.edu/%7Ejngiam/"> Jiquan Mgiam </a></li>
<li><a target="_blank" rel="nofollow" href="http://www-etud.iro.umontreal.ca/%7Eturian/"> Joseph Turian </a></li>
<li><a target="_blank" rel="nofollow" href="http://aclab.ca/users/josh/index.html"> Joshua Matthew Susskind </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.idsia.ch/%7Ejuergen/"> Jürgen Schmidhuber </a></li>
<li><a target="_blank" rel="nofollow" href="https://sites.google.com/site/blancousna/"> Justin A. Blanco </a></li>
<li><a target="_blank" rel="nofollow" href="http://koray.kavukcuoglu.org/"> Koray Kavukcuoglu </a></li>
<li><a target="_blank" rel="nofollow" href="http://users.ics.aalto.fi/kcho/"> KyungHyun Cho </a></li>
<li><a target="_blank" rel="nofollow" href="http://research.microsoft.com/en-us/people/deng/"> Li Deng </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.kyb.tuebingen.mpg.de/nc/employee/details/lucas.html"> Lucas Theis </a></li>
<li><a target="_blank" rel="nofollow" href="http://ludovicarnold.altervista.org/home/"> Ludovic Arnold </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.nyu.edu/%7Eranzato/"> Marc'Aurelio Ranzato </a></li>
<li><a target="_blank" rel="nofollow" href="http://aass.oru.se/%7Emlt/"> Martin Längkvist </a></li>
<li><a target="_blank" rel="nofollow" href="http://mdenil.com/"> Misha Denil </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Enorouzi/"> Mohammad Norouzi </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.ubc.ca/%7Enando/"> Nando de Freitas </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.utoronto.ca/%7Endjaitly/"> Navdeep Jaitly </a></li>
<li><a target="_blank" rel="nofollow" href="http://nicolas.le-roux.name/"> Nicolas Le Roux </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Enitish/"> Nitish Srivastava </a></li>
<li><a target="_blank" rel="nofollow" href="https://www.cisuc.uc.pt/people/show/2028"> Noel Lopes </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.berkeley.edu/%7Evinyals/"> Oriol Vinyals </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.iro.umontreal.ca/%7Evincentp"> Pascal Vincent </a></li>
<li><a target="_blank" rel="nofollow" href="https://sites.google.com/site/drpngx/"> Patrick Nguyen </a></li>
<li><a target="_blank" rel="nofollow" href="http://homes.cs.washington.edu/%7Epedrod/"> Pedro Domingos </a></li>
<li><a target="_blank" rel="nofollow" href="http://homepages.inf.ed.ac.uk/pseries/"> Peggy Series </a></li>
<li><a target="_blank" rel="nofollow" href="http://cs.nyu.edu/%7Esermanet"> Pierre Sermanet </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.nyu.edu/%7Emirowski/"> Piotr Mirowski </a></li>
<li><a target="_blank" rel="nofollow" href="http://ai.stanford.edu/%7Equocle/"> Quoc V. Le </a></li>
<li><a target="_blank" rel="nofollow" href="http://bci.tugraz.at/scherer/"> Reinhold Scherer </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.socher.org/"> Richard Socher </a></li>
<li><a target="_blank" rel="nofollow" href="http://cs.nyu.edu/%7Efergus/pmwiki/pmwiki.php"> Rob Fergus </a></li>
<li><a target="_blank" rel="nofollow" href="http://mil.engr.utk.edu/nmil/member/19.html"> Robert Coop </a></li>
<li><a target="_blank" rel="nofollow" href="http://homes.cs.washington.edu/%7Ercg/"> Robert Gens </a></li>
<li><a target="_blank" rel="nofollow" href="http://people.csail.mit.edu/rgrosse/"> Roger Grosse </a></li>
<li><a target="_blank" rel="nofollow" href="http://ronan.collobert.com/"> Ronan Collobert </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.utstat.toronto.edu/%7Ersalakhu/"> Ruslan Salakhutdinov </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.kyb.tuebingen.mpg.de/nc/employee/details/sgerwinn.html"> Sebastian Gerwinn </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cmap.polytechnique.fr/%7Emallat/"> Stéphane Mallat </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.ais.uni-bonn.de/behnke/"> Sven Behnke </a></li>
<li><a target="_blank" rel="nofollow" href="http://users.ics.aalto.fi/praiko/"> Tapani Raiko </a></li>
<li><a target="_blank" rel="nofollow" href="https://sites.google.com/site/tsainath/"> Tara Sainath </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Etijmen/"> Tijmen Tieleman </a></li>
<li><a target="_blank" rel="nofollow" href="http://mil.engr.utk.edu/nmil/member/36.html"> Tom Karnowski </a></li>
<li><a target="_blank" rel="nofollow" href="https://research.facebook.com/tomas-mikolov"> Tomáš Mikolov </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.idsia.ch/%7Emeier/"> Ueli Meier </a></li>
<li><a target="_blank" rel="nofollow" href="http://vincent.vanhoucke.com"> Vincent Vanhoucke </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Evmnih/"> Volodymyr Mnih </a></li>
<li><a target="_blank" rel="nofollow" href="http://yann.lecun.com/"> Yann LeCun </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Etang/"> Yichuan Tang </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/index.html"> Yoshua Bengio </a></li>
<li><a target="_blank" rel="nofollow" href="http://yota.ro/"> Yotaro Kubo </a></li>
<li><a target="_blank" rel="nofollow" href="http://ai.stanford.edu/%7Ewzou"> Youzhi (Will) Zou </a></li>
<li><a target="_blank" rel="nofollow" href="http://vision.stanford.edu/feifeili"> Fei-Fei Li </a></li>
<li><a target="_blank" rel="nofollow" href="https://research.google.com/pubs/105214.html"> Ian Goodfellow </a></li>
<li><a target="_blank" rel="nofollow" href="http://www.site.uottawa.ca/%7Elaganier/"> Robert Laganière </a></li>
</ol>
<h3><a href="#websites" aria-hidden="true" class="anchor" id="user-content-websites"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>WebSites</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.net/">deeplearning.net</a></li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.stanford.edu/">deeplearning.stanford.edu</a></li>
<li><a target="_blank" rel="nofollow" href="http://nlp.stanford.edu/">nlp.stanford.edu</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.ai-junkie.com/ann/evolved/nnt1.html">ai-junkie.com</a></li>
<li><a target="_blank" rel="nofollow" href="http://cs.brown.edu/research/ai/">cs.brown.edu/research/ai</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.eecs.umich.edu/ai/">eecs.umich.edu/ai</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.utexas.edu/users/ai-lab/">cs.utexas.edu/users/ai-lab</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.washington.edu/research/ai/">cs.washington.edu/research/ai</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.aiai.ed.ac.uk/">aiai.ed.ac.uk</a></li>
<li><a target="_blank" rel="nofollow" href="http://www-aig.jpl.nasa.gov/">www-aig.jpl.nasa.gov</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.csail.mit.edu/">csail.mit.edu</a></li>
<li><a target="_blank" rel="nofollow" href="http://cgi.cse.unsw.edu.au/%7Eaishare/">cgi.cse.unsw.edu.au/~aishare</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.rochester.edu/research/ai/">cs.rochester.edu/research/ai</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.ai.sri.com/">ai.sri.com</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.isi.edu/AI/isd.htm">isi.edu/AI/isd.htm</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.nrl.navy.mil/itd/aic/">nrl.navy.mil/itd/aic</a></li>
<li><a target="_blank" rel="nofollow" href="http://hips.seas.harvard.edu/">hips.seas.harvard.edu</a></li>
<li><a target="_blank" rel="nofollow" href="http://aiweekly.co">AI Weekly</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.stat.ucla.edu/%7Ejunhua.mao/m-RNN.html">stat.ucla.edu</a></li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.cs.toronto.edu/i2t">deeplearning.cs.toronto.edu</a></li>
<li><a target="_blank" rel="nofollow" href="http://jeffdonahue.com/lrcn/">jeffdonahue.com/lrcn/</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.visualqa.org/">visualqa.org</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/">www.mpi-inf.mpg.de/departments/computer-vision...</a></li>
<li><a target="_blank" rel="nofollow" href="http://news.startup.ml/">Deep Learning News</a></li>
<li><a target="_blank" rel="nofollow" href="https://medium.com/@ageitgey/">Machine Learning is Fun! Adam Geitgey's Blog</a></li>
<li><a target="_blank" rel="nofollow" href="http://yerevann.com/a-guide-to-deep-learning/">Guide to Machine Learning</a></li>
<li><a target="_blank" rel="nofollow" href="https://spandan-madan.github.io/DeepLearningProject/">Deep Learning for Beginners</a></li>
</ol>
<h3><a href="#datasets" aria-hidden="true" class="anchor" id="user-content-datasets"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Datasets</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> Handwritten digits</li>
<li><a target="_blank" rel="nofollow" href="http://ufldl.stanford.edu/housenumbers/">Google House Numbers</a> from street view</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Ekriz/cifar.html">CIFAR-10 and CIFAR-100</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.image-net.org/">IMAGENET</a></li>
<li><a target="_blank" rel="nofollow" href="http://groups.csail.mit.edu/vision/TinyImages/">Tiny Images</a> 80 Million tiny images6.</li>
<li><a target="_blank" rel="nofollow" href="https://yahooresearch.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images">Flickr Data</a> 100 Million Yahoo dataset</li>
<li><a target="_blank" rel="nofollow" href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/">Berkeley Segmentation Dataset 500</a></li>
<li><a target="_blank" rel="nofollow" href="http://archive.ics.uci.edu/ml/">UC Irvine Machine Learning Repository</a></li>
<li><a target="_blank" rel="nofollow" href="http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html">Flickr 8k</a></li>
<li><a target="_blank" rel="nofollow" href="http://shannon.cs.illinois.edu/DenotationGraph/">Flickr 30k</a></li>
<li><a target="_blank" rel="nofollow" href="http://mscoco.org/home/">Microsoft COCO</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.visualqa.org/">VQA</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.toronto.edu/%7Emren/imageqa/data/cocoqa/">Image QA</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.uk.research.att.com/facedatabase.html">AT&amp;T Laboratories Cambridge face database</a></li>
<li><a target="_blank" rel="nofollow" href="http://xtreme.gsfc.nasa.gov">AVHRR Pathfinder</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.anc.ed.ac.uk/%7Eamos/afreightdata.html">Air Freight</a> - The Air Freight data set is a ray-traced image sequence along with ground truth segmentation based on textural characteristics. (455 images + GT, each 160x120 pixels). (Formats: PNG)</li>
<li><a target="_blank" rel="nofollow" href="http://www.science.uva.nl/%7Ealoi/">Amsterdam Library of Object Images</a> - ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. (Formats: png)</li>
<li><a target="_blank" rel="nofollow" href="http://www.imm.dtu.dk/%7Eaam/">Annotated face, hand, cardiac &amp; meat images</a> - Most images &amp; annotations are supplemented by various ASM/AAM analyses using the AAM-API. (Formats: bmp,asf)</li>
<li><a target="_blank" rel="nofollow" href="http://www.imm.dtu.dk/image/">Image Analysis and Computer Graphics</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cog.brown.edu/%7Etarr/stimuli.html">Brown University Stimuli</a> - A variety of datasets including geons, objects, and "greebles". Good for testing recognition algorithms. (Formats: pict)</li>
<li><a target="_blank" rel="nofollow" href="http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/">CAVIAR video sequences of mall and public space behavior</a> - 90K video frames in 90 sequences of various human activities, with XML ground truth of detection and behavior classification (Formats: MPEG2 &amp; JPEG)</li>
<li><a target="_blank" rel="nofollow" href="http://www.ipab.inf.ed.ac.uk/mvu/">Machine Vision Unit</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.waikato.ac.nz/%7Esinglis/ccitt.html">CCITT Fax standard images</a> - 8 images (Formats: gif)</li>
<li><a target="_blank" rel="noopener noreferrer" href="https://github.com/ChristosChristofidis/awesome-deep-learning/blob/master/cil-ster.html">CMU CIL's Stereo Data with Ground Truth</a> - 3 sets of 11 images, including color tiff images with spectroradiometry (Formats: gif, tiff)</li>
<li><a target="_blank" rel="nofollow" href="http://www.ri.cmu.edu/projects/project_418.html">CMU PIE Database</a> - A database of 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 4 different expressions.</li>
<li><a target="_blank" rel="nofollow" href="http://www.ius.cs.cmu.edu/idb/">CMU VASC Image Database</a> - Images, sequences, stereo pairs (thousands of images) (Formats: Sun Rasterimage)</li>
<li><a target="_blank" rel="nofollow" href="http://www.vision.caltech.edu/html-files/archive.html">Caltech Image Database</a> - about 20 images - mostly top-down views of small objects and toys. (Formats: GIF)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.columbia.edu/CAVE/curet/">Columbia-Utrecht Reflectance and Texture Database</a> - Texture and reflectance measurements for over 60 samples of 3D texture, observed with over 200 different combinations of viewing and illumination directions. (Formats: bmp)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.sfu.ca/%7Ecolour/data/index.html">Computational Colour Constancy Data</a> - A dataset oriented towards computational color constancy, but useful for computer vision in general. It includes synthetic data, camera sensor data, and over 700 images. (Formats: tiff)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.sfu.ca/%7Ecolour/">Computational Vision Lab</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.washington.edu/research/imagedatabase/groundtruth/">Content-based image retrieval database</a> - 11 sets of color images for testing algorithms for content-based retrieval. Most sets have a description file with names of objects in each image. (Formats: jpg)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.washington.edu/research/imagedatabase/">Efficient Content-based Retrieval Group</a></li>
<li><a target="_blank" rel="nofollow" href="http://ls7-www.cs.uni-dortmund.de/%7Epeters/pages/research/modeladaptsys/modeladaptsys_vba_rov.html">Densely Sampled View Spheres</a> - Densely sampled view spheres - upper half of the view sphere of two toy objects with 2500 images each. (Formats: tiff)</li>
<li><a target="_blank" rel="nofollow" href="http://ls7-www.cs.uni-dortmund.de/">Computer Science VII (Graphical Systems)</a></li>
<li><a target="_blank" rel="nofollow" href="https://web-beta.archive.org/web/20011216051535/vision.psych.umn.edu/www/kersten-lab/demos/digitalembryo.html">Digital Embryos</a> - Digital embryos are novel objects which may be used to develop and test object recognition systems. They have an organic appearance. (Formats: various formats are available on request)</li>
<li><a target="_blank" rel="nofollow" href="http://vision.psych.umn.edu/www/kersten-lab/kersten-lab.html">Univerity of Minnesota Vision Lab</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.gastrointestinalatlas.com">El Salvador Atlas of Gastrointestinal VideoEndoscopy</a> - Images and Videos of his-res of studies taken from Gastrointestinal Video endoscopy. (Formats: jpg, mpg, gif)</li>
<li><a target="_blank" rel="nofollow" href="http://sting.cycollege.ac.cy/%7Ealanitis/fgnetaging/index.htm">FG-NET Facial Aging Database</a> - Database contains 1002 face images showing subjects at different ages. (Formats: jpg)</li>
<li><a target="_blank" rel="nofollow" href="http://bias.csr.unibo.it/fvc2000/">FVC2000 Fingerprint Databases</a> - FVC2000 is the First International Competition for Fingerprint Verification Algorithms. Four fingerprint databases constitute the FVC2000 benchmark (3520 fingerprints in all).</li>
<li><a target="_blank" rel="nofollow" href="http://bias.csr.unibo.it/research/biolab">Biometric Systems Lab</a> - University of Bologna</li>
<li><a target="_blank" rel="nofollow" href="http://www.fg-net.org">Face and Gesture images and image sequences</a> - Several image datasets of faces and gestures that are ground truth annotated for benchmarking</li>
<li><a target="_blank" rel="nofollow" href="http://www-i6.informatik.rwth-aachen.de/%7Edreuw/database.html">German Fingerspelling Database</a> - The database contains 35 gestures and consists of 1400 image sequences that contain gestures of 20 different persons recorded under non-uniform daylight lighting conditions. (Formats: mpg,jpg)</li>
<li><a target="_blank" rel="nofollow" href="http://www-i6.informatik.rwth-aachen.de/">Language Processing and Pattern Recognition</a></li>
<li><a target="_blank" rel="nofollow" href="http://hlab.phys.rug.nl/archive.html">Groningen Natural Image Database</a> - 4000+ 1536x1024 (16 bit) calibrated outdoor images (Formats: homebrew)</li>
<li><a target="_blank" rel="nofollow" href="http://www.icg.tu-graz.ac.at/%7Eschindler/Data">ICG Testhouse sequence</a> -  2 turntable sequences from ifferent viewing heights, 36 images each, resolution 1000x750, color (Formats: PPM)</li>
<li><a target="_blank" rel="nofollow" href="http://www.icg.tu-graz.ac.at">Institute of Computer Graphics and Vision</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.ien.it/is/vislib/">IEN Image Library</a> - 1000+ images, mostly outdoor sequences (Formats: raw, ppm)</li>
<li><a target="_blank" rel="nofollow" href="http://www-rocq.inria.fr/%7Etarel/syntim/images.html">INRIA's Syntim images database</a> - 15 color image of simple objects (Formats: gif)</li>
<li><a target="_blank" rel="nofollow" href="http://www.inria.fr/">INRIA</a></li>
<li><a target="_blank" rel="nofollow" href="http://www-rocq.inria.fr/%7Etarel/syntim/paires.html">INRIA's Syntim stereo databases</a> - 34 calibrated color stereo pairs (Formats: gif)</li>
<li><a target="_blank" rel="nofollow" href="http://www.ece.ncsu.edu/imaging/Archives/ImageDataBase/index.html">Image Analysis Laboratory</a> - Images obtained from a variety of imaging modalities -- raw CFA images, range images and a host of "medical images". (Formats: homebrew)</li>
<li><a target="_blank" rel="nofollow" href="http://www.ece.ncsu.edu/imaging">Image Analysis Laboratory</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.prip.tuwien.ac.at/prip/image.html">Image Database</a> - An image database including some textures</li>
<li><a target="_blank" rel="nofollow" href="http://www.mis.atr.co.jp/%7Emlyons/jaffe.html">JAFFE Facial Expression Image Database</a> - The JAFFE database consists of 213 images of Japanese female subjects posing 6 basic facial expressions as well as a neutral pose. Ratings on emotion adjectives are also available, free of charge, for research purposes. (Formats: TIFF Grayscale images.)</li>
<li><a target="_blank" rel="nofollow" href="http://www.mic.atr.co.jp/">ATR Research, Kyoto, Japan</a></li>
<li>JISCT Stereo Evaluation - 44 image pairs. These data have been used in an evaluation of stereo analysis, as described in the April 1993 ARPA Image Understanding Workshop paper ``The JISCT Stereo Evaluation'' by R.C.Bolles, H.H.Baker, and M.J.Hannah, 263--274 (Formats: SSI)</li>
<li><a target="_blank" rel="nofollow" href="http://www-white.media.mit.edu/vismod/imagery/VisionTexture/vistex.html">MIT Vision Texture</a> - Image archive (100+ images) (Formats: ppm)</li>
<li>MIT face images and more - hundreds of images (Formats: homebrew)</li>
<li><a target="_blank" rel="nofollow" href="http://vision.cse.psu.edu/book/testbed/images/">Machine Vision</a> - Images from the textbook by Jain, Kasturi, Schunck (20+ images) (Formats: GIF TIFF)</li>
<li><a target="_blank" rel="nofollow" href="http://marathon.csee.usf.edu/Mammography/Database.html">Mammography Image Databases</a> - 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided. (Formats: homebrew)</li>
<li><a target="_blank" rel="noopener noreferrer" href="ftp://ftp.cps.msu.edu/pub/prip">ftp://ftp.cps.msu.edu/pub/prip</a> - many images (Formats: unknown)</li>
<li><a target="_blank" rel="nofollow" href="http://www.middlebury.edu/stereo/data.html">Middlebury Stereo Data Sets with Ground Truth</a> - Six multi-frame stereo data sets of scenes containing planar regions. Each data set contains 9 color images and subpixel-accuracy ground-truth data. (Formats: ppm)</li>
<li><a target="_blank" rel="nofollow" href="http://www.middlebury.edu/stereo">Middlebury Stereo Vision Research Page</a> - Middlebury College</li>
<li><a target="_blank" rel="nofollow" href="http://ltpwww.gsfc.nasa.gov/MODIS/MAS/">Modis Airborne simulator, Gallery and data set</a> - High Altitude Imagery from around the world for environmental modeling in support of NASA EOS program (Formats: JPG and HDF)</li>
<li>NIST Fingerprint and handwriting - datasets - thousands of images (Formats: unknown)</li>
<li>NIST Fingerprint data - compressed multipart uuencoded tar file</li>
<li><a target="_blank" rel="nofollow" href="http://www.nlm.nih.gov/research/visible/visible_human.html">NLM HyperDoc Visible Human Project</a> - Color, CAT and MRI image samples - over 30 images (Formats: jpeg)</li>
<li><a target="_blank" rel="nofollow" href="http://www.designrepository.org">National Design Repository</a> - Over 55,000 3D CAD and solid models of (mostly) mechanical/machined engineerign designs. (Formats: gif,vrml,wrl,stp,sat)</li>
<li><a target="_blank" rel="nofollow" href="http://gicl.mcs.drexel.edu">Geometric &amp; Intelligent Computing Laboratory</a></li>
<li><a target="_blank" rel="nofollow" href="http://eewww.eng.ohio-state.edu/%7Eflynn/3DDB/Models/">OSU (MSU) 3D Object Model Database</a> - several sets of 3D object models collected over several years to use in object recognition research (Formats: homebrew, vrml)</li>
<li><a target="_blank" rel="nofollow" href="http://eewww.eng.ohio-state.edu/%7Eflynn/3DDB/RID/">OSU (MSU/WSU) Range Image Database</a> - Hundreds of real and synthetic images (Formats: gif, homebrew)</li>
<li><a target="_blank" rel="nofollow" href="http://sampl.eng.ohio-state.edu/%7Esampl/database.htm">OSU/SAMPL Database: Range Images, 3D Models, Stills, Motion Sequences</a> - Over 1000 range images, 3D object models, still images and motion sequences (Formats: gif, ppm, vrml, homebrew)</li>
<li><a target="_blank" rel="nofollow" href="http://sampl.eng.ohio-state.edu">Signal Analysis and Machine Perception Laboratory</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.otago.ac.nz/research/vision/Research/OpticalFlow/opticalflow.html">Otago Optical Flow Evaluation Sequences</a> - Synthetic and real sequences with machine-readable ground truth optical flow fields, plus tools to generate ground truth for new sequences. (Formats: ppm,tif,homebrew)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.otago.ac.nz/research/vision/index.html">Vision Research Group</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/">ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/</a> - Real and synthetic image sequences used for testing a Particle Image Velocimetry application. These images may be used for the test of optical flow and image matching algorithms. (Formats: pgm (raw))</li>
<li><a target="_blank" rel="nofollow" href="http://www.limsi.fr/Recherche/IMM/PageIMM.html">LIMSI-CNRS/CHM/IMM/vision</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.limsi.fr/">LIMSI-CNRS</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.taurusstudio.net/research/pmtexdb/index.htm">Photometric 3D Surface Texture Database</a> - This is the first 3D texture database which provides both full real surface rotations and registered photometric stereo data (30 textures, 1680 images). (Formats: TIFF)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cee.hw.ac.uk/%7Emtc/sofa">SEQUENCES FOR OPTICAL FLOW ANALYSIS (SOFA)</a> - 9 synthetic sequences designed for testing motion analysis applications, including full ground truth of motion and camera parameters. (Formats: gif)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cee.hw.ac.uk/%7Emtc/research.html">Computer Vision Group</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.nada.kth.se/%7Ezucch/CAMERA/PUB/seq.html">Sequences for Flow Based Reconstruction</a> - synthetic sequence for testing structure from motion algorithms (Formats: pgm)</li>
<li><a target="_blank" rel="nofollow" href="http://www-dbv.cs.uni-bonn.de/stereo_data/">Stereo Images with Ground Truth Disparity and Occlusion</a> - a small set of synthetic images of a hallway with varying amounts of noise added. Use these images to benchmark your stereo algorithm. (Formats: raw, viff (khoros), or tiff)</li>
<li><a target="_blank" rel="nofollow" href="http://range.informatik.uni-stuttgart.de">Stuttgart Range Image Database</a> - A collection of synthetic range images taken from high-resolution polygonal models available on the web (Formats: homebrew)</li>
<li><a target="_blank" rel="nofollow" href="http://www.informatik.uni-stuttgart.de/ipvr/bv/bv_home_engl.html">Department Image Understanding</a></li>
<li><a target="_blank" rel="nofollow" href="http://www2.ece.ohio-state.edu/%7Ealeix/ARdatabase.html">The AR Face Database</a> - Contains over 4,000 color images corresponding to 126 people's faces (70 men and 56 women). Frontal views with variations in facial expressions, illumination, and occlusions. (Formats: RAW (RGB 24-bit))</li>
<li><a target="_blank" rel="nofollow" href="http://rvl.www.ecn.purdue.edu/RVL/">Purdue Robot Vision Lab</a></li>
<li><a target="_blank" rel="nofollow" href="http://web.mit.edu/torralba/www/database.html">The MIT-CSAIL Database of Objects and Scenes</a> - Database for testing multiclass object detection and scene recognition algorithms. Over 72,000 images with 2873 annotated frames. More than 50 annotated object classes. (Formats: jpg)</li>
<li><a target="_blank" rel="nofollow" href="http://rvl1.ecn.purdue.edu/RVL/specularity_database/">The RVL SPEC-DB (SPECularity DataBase)</a> - A collection of over 300 real images of 100 objects taken under three different illuminaiton conditions (Diffuse/Ambient/Directed). -- Use these images to test algorithms for detecting and compensating specular highlights in color images. (Formats: TIFF )</li>
<li><a target="_blank" rel="nofollow" href="http://rvl1.ecn.purdue.edu/RVL/">Robot Vision Laboratory</a></li>
<li><a target="_blank" rel="nofollow" href="http://xm2vtsdb.ee.surrey.ac.uk">The Xm2vts database</a> - The XM2VTSDB contains four digital recordings of 295 people taken over a period of four months. This database contains both image and video data of faces.</li>
<li><a target="_blank" rel="nofollow" href="http://www.ee.surrey.ac.uk/Research/CVSSP">Centre for Vision, Speech and Signal Processing</a></li>
<li><a target="_blank" rel="nofollow" href="http://i21www.ira.uka.de/image_sequences">Traffic Image Sequences and 'Marbled Block' Sequence</a> - thousands of frames of digitized traffic image sequences as well as the 'Marbled Block' sequence (grayscale images) (Formats: GIF)</li>
<li><a target="_blank" rel="nofollow" href="http://i21www.ira.uka.de">IAKS/KOGS</a></li>
<li>U Bern Face images - hundreds of images (Formats: Sun rasterfile)</li>
<li>U Michigan textures (Formats: compressed raw)</li>
<li><a target="_blank" rel="nofollow" href="http://www.ee.oulu.fi/%7Eolli/Projects/Lumber.Grading.html">U Oulu wood and knots database</a> - Includes classifications - 1000+ color images (Formats: ppm)</li>
<li><a target="_blank" rel="nofollow" href="http://vision.doc.ntu.ac.uk/datasets/UCID/ucid.html">UCID - an Uncompressed Colour Image Database</a> - a benchmark database for image retrieval with predefined ground truth. (Formats: tiff)</li>
<li><a target="_blank" rel="nofollow" href="http://vis-www.cs.umass.edu/%7Evislib/">UMass Vision Image Archive</a> - Large image database with aerial, space, stereo, medical images and more. (Formats: homebrew)</li>
<li>UNC's 3D image database - many images (Formats: GIF)</li>
<li><a target="_blank" rel="nofollow" href="http://marathon.csee.usf.edu/range/seg-comp/SegComp.html">USF Range Image Data with Segmentation Ground Truth</a> - 80 image sets (Formats: Sun rasterimage)</li>
<li><a target="_blank" rel="nofollow" href="http://www.ee.oulu.fi/research/imag/color/pbfd.html">University of Oulu Physics-based Face Database</a> - contains color images of faces under different illuminants and camera calibration conditions as well as skin spectral reflectance measurements of each person.</li>
<li><a target="_blank" rel="nofollow" href="http://www.ee.oulu.fi/mvmp/">Machine Vision and Media Processing Unit</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.outex.oulu.fi">University of Oulu Texture Database</a> - Database of 320 surface textures, each captured under three illuminants, six spatial resolutions and nine rotation angles. A set of test suites is also provided so that texture segmentation, classification, and retrieval algorithms can be tested in a standard manner. (Formats: bmp, ras, xv)</li>
<li><a target="_blank" rel="nofollow" href="http://www.ee.oulu.fi/mvg">Machine Vision Group</a></li>
<li>Usenix face database - Thousands of face images from many different sites (circa 994)</li>
<li><a target="_blank" rel="nofollow" href="http://www-prima.inrialpes.fr/Prima/hall/view_sphere.html">View Sphere Database</a> - Images of 8 objects seen from many different view points. The view sphere is sampled using a geodesic with 172 images/sphere. Two sets for training and testing are available. (Formats: ppm)</li>
<li><a target="_blank" rel="nofollow" href="http://www-prima.inrialpes.fr/Prima/">PRIMA, GRAVIR</a></li>
<li>Vision-list Imagery Archive - Many images, many formats</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.cmu.edu/%7Eowenc/word.htm">Wiry Object Recognition Database</a> - Thousands of images of a cart, ladder, stool, bicycle, chairs, and cluttered scenes with ground truth labelings of edges and regions. (Formats: jpg)</li>
<li><a target="_blank" rel="nofollow" href="http://www.cs.cmu.edu/0.000000E+003dvision/">3D Vision Group</a></li>
<li><a target="_blank" rel="nofollow" href="http://cvc.yale.edu/projects/yalefaces/yalefaces.html">Yale Face Database</a> -  165 images (15 individuals) with different lighting, expression, and occlusion configurations.</li>
<li><a target="_blank" rel="nofollow" href="http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html">Yale Face Database B</a> - 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions). (Formats: PGM)</li>
<li><a target="_blank" rel="nofollow" href="http://cvc.yale.edu/">Center for Computational Vision and Control</a></li>
<li><a id="user-content-deepmind/rc-data" target="_blank" rel="noopener noreferrer" href="https://github.com/deepmind/rc-data">DeepMind QA Corpus</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">832 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">154 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span> - Textual QA corpus from CNN and DailyMail. More than 300K documents in total. <a target="_blank" rel="nofollow" href="http://arxiv.org/abs/1506.03340">Paper</a> for reference.</li>
<li><a target="_blank" rel="nofollow" href="https://research.google.com/youtube8m/">YouTube-8M Dataset</a> - YouTube-8M is a large-scale labeled video dataset that consists of 8 million YouTube video IDs and associated labels from a diverse vocabulary of 4800 visual entities.</li>
<li><a id="user-content-openimages/dataset" target="_blank" rel="noopener noreferrer" href="https://github.com/openimages/dataset">Open Images dataset</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">338 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span> - Open Images is a dataset of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories.</li>
<li><a target="_blank" rel="nofollow" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit">Visual Object Classes Challenge 2012 (VOC2012)</a> - VOC2012 dataset containing 12k images with 20 annotated classes for object detection and segmentation.</li>
<li><a id="user-content-zalandoresearch/fashion-mnist" target="_blank" rel="noopener noreferrer" href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">352 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span> - MNIST like fashion product dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.</li>
<li><a target="_blank" rel="nofollow" href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html">Large-scale Fashion (DeepFashion) Database</a> - Contains over 800,000 diverse fashion images.  Each image in this dataset is labeled with 50 categories, 1,000 descriptive attributes, bounding box and clothing landmarks</li>
</ol>
<h3><a href="#conferences" aria-hidden="true" class="anchor" id="user-content-conferences"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conferences</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="http://cvpr2018.thecvf.com">CVPR - IEEE Conference on Computer Vision and Pattern Recognition</a></li>
<li><a target="_blank" rel="nofollow" href="http://celweb.vuse.vanderbilt.edu/aamas18/">AAMAS - International Joint Conference on Autonomous Agents and Multiagent Systems</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.ijcai-18.org/">IJCAI - 	International Joint Conference on Artificial Intelligence</a></li>
<li><a target="_blank" rel="nofollow" href="https://icml.cc">ICML - 	International Conference on Machine Learning</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.ecmlpkdd2018.org">ECML - European Conference on Machine Learning</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.kdd.org/kdd2018/">KDD - Knowledge Discovery and Data Mining</a></li>
<li><a target="_blank" rel="nofollow" href="https://nips.cc/Conferences/2018">NIPS - Neural Information Processing Systems</a></li>
<li><a target="_blank" rel="nofollow" href="https://conferences.oreilly.com/artificial-intelligence/ai-ny">O'Reilly AI Conference - 	O'Reilly Artificial Intelligence Conference</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.waset.org/conference/2018/07/istanbul/ICDM">ICDM - International Conference on Data Mining</a></li>
<li><a target="_blank" rel="nofollow" href="http://iccv2017.thecvf.com">ICCV - International Conference on Computer Vision</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.aaai.org">AAAI - Association for the Advancement of Artificial Intelligence</a></li>
</ol>
<h3><a href="#frameworks" aria-hidden="true" class="anchor" id="user-content-frameworks"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Frameworks</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="http://caffe.berkeleyvision.org/">Caffe</a></li>
<li><a target="_blank" rel="nofollow" href="http://torch.ch/">Torch7</a></li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning.net/software/theano/">Theano</a></li>
<li><a target="_blank" rel="nofollow" href="https://code.google.com/p/cuda-convnet2/">cuda-convnet</a></li>
<li><a id="user-content-karpathy/convnetjs" target="_blank" rel="noopener noreferrer" href="https://github.com/karpathy/convnetjs">convetjs</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">349 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="http://libccv.org/doc/doc-convnet/">Ccv</a></li>
<li><a target="_blank" rel="nofollow" href="http://numenta.org/nupic.html">NuPIC</a></li>
<li><a target="_blank" rel="nofollow" href="http://deeplearning4j.org/">DeepLearning4J</a></li>
<li><a id="user-content-harthur/brain" target="_blank" rel="noopener noreferrer" href="https://github.com/harthur/brain">Brain</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">5k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">464 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-rasmusbergpalm/DeepLearnToolbox" target="_blank" rel="noopener noreferrer" href="https://github.com/rasmusbergpalm/DeepLearnToolbox">DeepLearnToolbox</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-nitishsrivastava/deepnet" target="_blank" rel="noopener noreferrer" href="https://github.com/nitishsrivastava/deepnet">Deepnet</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">393 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">204 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-andersbll/deeppy" target="_blank" rel="noopener noreferrer" href="https://github.com/andersbll/deeppy">Deeppy</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">269 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-ivan-vasilev/neuralnetworks" target="_blank" rel="noopener noreferrer" href="https://github.com/ivan-vasilev/neuralnetworks">JavaNN</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">438 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">131 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-hannes-brt/hebel" target="_blank" rel="noopener noreferrer" href="https://github.com/hannes-brt/hebel">hebel</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">136 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-pluskid/Mocha.jl" target="_blank" rel="noopener noreferrer" href="https://github.com/pluskid/Mocha.jl">Mocha.jl</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">999 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">260 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-guoding83128/OpenDL" target="_blank" rel="noopener noreferrer" href="https://github.com/guoding83128/OpenDL">OpenDL</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">134 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">70 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="https://developer.nvidia.com/cuDNN">cuDNN</a></li>
<li><a target="_blank" rel="nofollow" href="http://melisgl.github.io/mgl-pax-world/mgl-manual.html">MGL</a></li>
<li><a id="user-content-denizyuret/Knet.jl" target="_blank" rel="noopener noreferrer" href="https://github.com/denizyuret/Knet.jl">Knet.jl</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">384 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">104 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-NVIDIA/DIGITS" target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/DIGITS">Nvidia DIGITS - a web app based on Caffe</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-NervanaSystems/neon" target="_blank" rel="noopener noreferrer" href="https://github.com/NervanaSystems/neon">Neon - Python based Deep Learning Framework</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">4k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">887 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="http://keras.io">Keras - Theano based Deep Learning Library</a></li>
<li><a target="_blank" rel="nofollow" href="http://chainer.org/">Chainer - A flexible framework of neural networks for deep learning</a></li>
<li><a target="_blank" rel="nofollow" href="http://rnnlm.org/">RNNLM Toolkit</a></li>
<li><a target="_blank" rel="nofollow" href="http://sourceforge.net/p/rnnl/wiki/Home/">RNNLIB - A recurrent neural network library</a></li>
<li><a id="user-content-karpathy/char-rnn" target="_blank" rel="noopener noreferrer" href="https://github.com/karpathy/char-rnn">char-rnn</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">6k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-vlfeat/matconvnet" target="_blank" rel="noopener noreferrer" href="https://github.com/vlfeat/matconvnet">MatConvNet: CNNs for MATLAB</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">913 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">624 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-dmlc/minerva" target="_blank" rel="noopener noreferrer" href="https://github.com/dmlc/minerva">Minerva - a fast and flexible tool for deep learning on multi-GPU</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">444 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">128 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-IDSIA/brainstorm" target="_blank" rel="noopener noreferrer" href="https://github.com/IDSIA/brainstorm">Brainstorm - Fast, flexible and fun neural networks.</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">158 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-tensorflow/tensorflow" target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/tensorflow">Tensorflow - Open source software library for numerical computation using data flow graphs</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">100k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">52k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-Microsoft/DMTK" target="_blank" rel="noopener noreferrer" href="https://github.com/Microsoft/DMTK">DMTK - Microsoft Distributed Machine Learning Tookit</a></li>
<li><a id="user-content-google/skflow" target="_blank" rel="noopener noreferrer" href="https://github.com/google/skflow">Scikit Flow - Simplified interface for TensorFlow (mimicking Scikit Learn)</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">144 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-dmlc/mxnet" target="_blank" rel="noopener noreferrer" href="https://github.com/dmlc/mxnet/">MXnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning framework</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">12k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">5k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-Samsung/veles" target="_blank" rel="noopener noreferrer" href="https://github.com/Samsung/veles">Veles - Samsung Distributed machine learning platform</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">872 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">199 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-PrincetonVision/marvin" target="_blank" rel="noopener noreferrer" href="https://github.com/PrincetonVision/marvin">Marvin - A Minimalist GPU-only N-Dimensional ConvNets Framework</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">387 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">146 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="http://singa.incubator.apache.org/">Apache SINGA - A General Distributed Deep Learning Platform</a></li>
<li><a id="user-content-amznlabs/amazon-dsstne" target="_blank" rel="noopener noreferrer" href="https://github.com/amznlabs/amazon-dsstne">DSSTNE - Amazon's library for building Deep Learning models</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">8k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-tensorflow/models" target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/syntaxnet">SyntaxNet - Google's syntactic parser - A TensorFlow dependency library</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">30k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">16k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="http://mlpack.org/">mlpack - A scalable Machine Learning library</a></li>
<li><a id="user-content-torchnet/torchnet" target="_blank" rel="noopener noreferrer" href="https://github.com/torchnet/torchnet">Torchnet - Torch based Deep Learning Library</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">963 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">219 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-baidu/paddle" target="_blank" rel="noopener noreferrer" href="https://github.com/baidu/paddle">Paddle - PArallel Distributed Deep LEarning by Baidu</a></li>
<li><a target="_blank" rel="nofollow" href="http://neupy.com">NeuPy - Theano based Python library for ANN and Deep Learning</a></li>
<li><a id="user-content-Lasagne/Lasagne" target="_blank" rel="noopener noreferrer" href="https://github.com/Lasagne/Lasagne">Lasagne - a lightweight library to build and train neural networks in Theano</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">956 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-dnouri/nolearn" target="_blank" rel="noopener noreferrer" href="https://github.com/dnouri/nolearn">nolearn - wrappers and abstractions around existing neural network libraries, most notably Lasagne</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">920 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">280 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-deepmind/sonnet" target="_blank" rel="noopener noreferrer" href="https://github.com/deepmind/sonnet">Sonnet - a library for constructing neural networks by Google's DeepMind</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">186 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-pytorch/pytorch" target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/pytorch">PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">13k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-Microsoft/CNTK" target="_blank" rel="noopener noreferrer" href="https://github.com/Microsoft/CNTK">CNTK - Microsoft Cognitive Toolkit</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">16k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">5k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-SerpentAI/SerpentAI" target="_blank" rel="noopener noreferrer" href="https://github.com/SerpentAI/SerpentAI">Serpent.AI - Game agent framework: Use any video game as a deep learning sandbox</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">240 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-caffe2/caffe2" target="_blank" rel="noopener noreferrer" href="https://github.com/caffe2/caffe2">Caffe2 - A New Lightweight, Modular, and Scalable Deep Learning Framework</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">10k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-PAIR-code/deeplearnjs" target="_blank" rel="noopener noreferrer" href="https://github.com/PAIR-code/deeplearnjs">deeplearn.js - Hardware-accelerated deep learning and linear algebra (NumPy) library for the web</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">6k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">570 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
</ol>
<h3><a href="#miscellaneous" aria-hidden="true" class="anchor" id="user-content-miscellaneous"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Miscellaneous</h3>
<ol>
<li><a target="_blank" rel="nofollow" href="https://plus.google.com/communities/112866381580457264725">Google Plus - Deep Learning Community</a></li>
<li><a target="_blank" rel="nofollow" href="http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=shelhamer&amp;searchItems=&amp;sessionTopic=&amp;sessionEvent=4&amp;sessionYear=2014&amp;sessionFormat=&amp;submit=&amp;select=+">Caffe Webinar</a></li>
<li><a target="_blank" rel="nofollow" href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/">100 Best Github Resources in Github for DL</a></li>
<li><a target="_blank" rel="nofollow" href="https://code.google.com/p/word2vec/">Word2Vec</a></li>
<li><a id="user-content-tleyden/docker" target="_blank" rel="noopener noreferrer" href="https://github.com/tleyden/docker/tree/master/caffe">Caffe DockerFile</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-grey-dark">13 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-grey-dark">7 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-TorontoDeepLearning/convnet" target="_blank" rel="noopener noreferrer" href="https://github.com/TorontoDeepLearning/convnet">TorontoDeepLEarning convnet</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">279 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">124 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-clementfarabet/gfx.js" target="_blank" rel="noopener noreferrer" href="https://github.com/clementfarabet/gfx.js">gfx.js</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">30 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-grey-dark">17 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-torch/torch7" target="_blank" rel="noopener noreferrer" href="https://github.com/torch/torch7/wiki/Cheatsheet">Torch7 Cheat sheet</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">8k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/">Misc from MIT's 'Advanced Natural Language Processing' course</a></li>
<li><a target="_blank" rel="nofollow" href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/">Misc from MIT's 'Machine Learning' course</a></li>
<li><a target="_blank" rel="nofollow" href="http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-520-a-networks-for-learning-regression-and-classification-spring-2001/">Misc from MIT's 'Networks for Learning: Regression and Classification' course</a></li>
<li><a target="_blank" rel="nofollow" href="http://ocw.mit.edu/courses/health-sciences-and-technology/hst-723j-neural-coding-and-perception-of-sound-spring-2005/index.htm">Misc from MIT's 'Neural Coding and Perception of Sound' course</a></li>
<li><a target="_blank" rel="nofollow" href="http://www.datasciencecentral.com/profiles/blogs/implementing-a-distributed-deep-learning-network-over-spark">Implementing a Distributed Deep Learning Network over Spark</a></li>
<li><a id="user-content-erikbern/deep-pink" target="_blank" rel="noopener noreferrer" href="https://github.com/erikbern/deep-pink">A chess AI that learns to play chess using deep learning.</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">218 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">37 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-kristjankorjus/Replicating-DeepMind" target="_blank" rel="noopener noreferrer" href="https://github.com/kristjankorjus/Replicating-DeepMind">Reproducing the results of "Playing Atari with Deep Reinforcement Learning" by DeepMind</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">215 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">62 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-idio/wiki2vec" target="_blank" rel="noopener noreferrer" href="https://github.com/idio/wiki2vec">Wiki2Vec. Getting Word2vec vectors for entities and word from Wikipedia Dumps</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">375 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">99 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-kuz/DeepMind-Atari-Deep-Q-Learner" target="_blank" rel="noopener noreferrer" href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner">The original code from the DeepMind article + tweaks</a></li>
<li><a id="user-content-google/deepdream" target="_blank" rel="noopener noreferrer" href="https://github.com/google/deepdream">Google deepdream - Neural Network art</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">11k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">3k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-gist.karpathy/587454dc0146a6ae21fc" target="_blank" rel="noopener noreferrer" href="https://gist.github.com/karpathy/587454dc0146a6ae21fc">An efficient, batched LSTM.</a></li>
<li><a id="user-content-hexahedria/biaxial-rnn-music-composition" target="_blank" rel="noopener noreferrer" href="https://github.com/hexahedria/biaxial-rnn-music-composition">A recurrent neural network designed to generate classical music.</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">263 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-facebook/MemNN" target="_blank" rel="noopener noreferrer" href="https://github.com/facebook/MemNN">Memory Networks Implementations - Facebook</a></li>
<li><a id="user-content-cmusatyalab/openface" target="_blank" rel="noopener noreferrer" href="https://github.com/cmusatyalab/openface">Face recognition with Google's FaceNet deep neural network.</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">7k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-joeledenberg/DigitRecognition" target="_blank" rel="noopener noreferrer" href="https://github.com/joeledenberg/DigitRecognition">Basic digit recognition neural network</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-grey-dark">11 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-grey-dark">10 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="https://www.projectoxford.ai/demo/emotion#detection">Emotion Recognition API Demo - Microsoft</a></li>
<li><a id="user-content-ethereon/caffe-tensorflow" target="_blank" rel="noopener noreferrer" href="https://github.com/ethereon/caffe-tensorflow">Proof of concept for loading Caffe models in TensorFlow</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">1k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">418 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="http://pjreddie.com/darknet/yolo/#webcam">YOLO: Real-Time Object Detection</a></li>
<li><a id="user-content-Rochester-NRT/AlphaGo" target="_blank" rel="noopener noreferrer" href="https://github.com/Rochester-NRT/AlphaGo">AlphaGo - A replication of DeepMind's 2016 Nature publication, "Mastering the game of Go with deep neural networks and tree search"</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">7k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a id="user-content-ZuzooVn/machine-learning-for-software-engineers" target="_blank" rel="noopener noreferrer" href="https://github.com/ZuzooVn/machine-learning-for-software-engineers">Machine Learning for Software Engineers</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-orange-dark">15k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-green-dark">2k <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span></li>
<li><a target="_blank" rel="nofollow" href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.oa4rzez3g">Machine Learning is Fun!</a></li>
<li><a target="_blank" rel="nofollow" href="https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A">Siraj Raval's Deep Learning tutorials</a></li>
<li><a id="user-content-natanielruiz/dockerface" target="_blank" rel="noopener noreferrer" href="https://github.com/natanielruiz/dockerface">Dockerface</a> - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.</li>
<li><a id="user-content-ybayle/awesome-deep-learning-music" target="_blank" rel="noopener noreferrer" href="https://github.com/ybayle/awesome-deep-learning-music">Awesome Deep Learning Music</a><span class="ml-1 text-grey-dark whitespace-no-wrap"><span class="text-grey-dark">8 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 13" width="14" height="12" class="mr-1"><polygon points="14 5 9.1 4.36 7 0 4.9 4.36 0 5 3.6 8.26 2.67 13 7 10.67 11.33 13 10.4 8.26"></polygon></svg></span><span class="text-grey-dark">1 <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 14" width="10" height="12"><path d="M8,-8.47032947e-22 C7.09660216,-0.00238695453 6.3045861,0.603179379 6.07006224,1.47560813 C5.83553838,2.34803688 6.21717019,3.26909995 7,3.72 L7,5 L5,7 L3,5 L3,3.72 C3.78282981,3.26909995 4.16446162,2.34803688 3.92993776,1.47560813 C3.6954139,0.603179379 2.90339784,-0.00238695453 2,-4.44088363e-16 C1.09660216,-0.00238695453 0.304586097,0.603179379 0.0700622397,1.47560813 C-0.164461618,2.34803688 0.217170191,3.26909995 1,3.72 L1,5.5 L4,8.5 L4,10.28 C3.21717019,10.7309 2.83553838,11.6519631 3.07006224,12.5243919 C3.3045861,13.3968206 4.09660216,14.002387 5,14 C5.90339784,14.002387 6.6954139,13.3968206 6.92993776,12.5243919 C7.16446162,11.6519631 6.78282981,10.7309 6,10.28 L6,8.5 L9,5.5 L9,3.72 C9.78282981,3.26909995 10.1644616,2.34803688 9.92993776,1.47560813 C9.6954139,0.603179379 8.90339784,-0.00238695453 8,-8.47032947e-22 Z M2,3.2 C1.34,3.2 0.8,2.65 0.8,2 C0.8,1.35 1.35,0.8 2,0.8 C2.65,0.8 3.2,1.35 3.2,2 C3.2,2.65 2.65,3.2 2,3.2 Z M5,13.2 C4.34,13.2 3.8,12.65 3.8,12 C3.8,11.35 4.35,10.8 5,10.8 C5.65,10.8 6.2,11.35 6.2,12 C6.2,12.65 5.65,13.2 5,13.2 Z M8,3.2 C7.34,3.2 6.8,2.65 6.8,2 C6.8,1.35 7.35,0.8 8,0.8 C8.65,0.8 9.2,1.35 9.2,2 C9.2,2.65 8.65,3.2 8,3.2 Z"></path></svg></span></span> - Curated list of articles related to deep learning scientific research applied to music</li>
</ol>
<hr>
<h3><a href="#contributing" aria-hidden="true" class="anchor" id="user-content-contributing"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contributing</h3>
<p>Have anything in mind that you think is awesome and would fit in this list? Feel free to send a <a id="user-content-ashara12/awesome-deeplearning" target="_blank" rel="noopener noreferrer" href="https://github.com/ashara12/awesome-deeplearning/pulls">pull request</a>.</p>
<hr>
<h2><a href="#license" aria-hidden="true" class="anchor" id="user-content-license"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>License</h2>
<p><a target="_blank" rel="nofollow" href="http://creativecommons.org/publicdomain/zero/1.0/"><img alt="CC0" target="_blank" rel="noopener noreferrer" src="https://camo.githubusercontent.com/c5160f944848828fa33126d9a697e9abe43ea98f/687474703a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f702f7a65726f2f312e302f38387833312e706e67" data-canonical-src="http://i.creativecommons.org/p/zero/1.0/88x31.png" style="max-width:100%"></a></p>
<p>To the extent possible under law, <a target="_blank" rel="nofollow" href="https://linkedin.com/in/Christofidis">Christos Christofidis</a> has waived all copyright and related or neighboring rights to this work.</p>
</div></div></div><div class="w-full max-w-xs bg-grey-lightest fixed pin-r pin-t pin-b mt-15 lg:mt-0 lg:flex flex-col overflow-hidden z-20 hidden"><div class="flex-1 overflow-y-scroll scrolling-touch -mr-8 pr-8"><div class="m-4 text-grey leading-loose text-center text-sm"><noscript>Enable JavaScript for more features</noscript></div></div><div class="flex-none"><div class="ad border-t shadow-lg hidden lg:block p-4"></div></div></div><div style="position:fixed;bottom:30px;right:350px;cursor:pointer;transition:opacity .2s linear 0s,visibility;z-index:20;opacity:0;visibility:hidden"><div class="bg-grey-light p-2 rounded hidden lg:block"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 16" class="block" width="32" height="32"><path fill-rule="evenodd" d="M5 3L0 9h3v4h4V9h3z"></path></svg></div></div></div></div><script type="text/javascript" src="/static/js/main.ce8adbb6.js"></script></body></html>